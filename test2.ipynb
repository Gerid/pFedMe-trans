{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c99602d590>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from kmeans_pytorch import kmeans\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "import FLAlgorithms\n",
    "import utils\n",
    "\n",
    "#for jupyter notebook reload modules\n",
    "import imp\n",
    "imp.reload(FLAlgorithms) \n",
    "imp.reload(utils) \n",
    "from FLAlgorithms.servers.serveravg import FedAvg\n",
    "from FLAlgorithms.servers.serverpFedMe import pFedMe\n",
    "from FLAlgorithms.servers.serverperavg import PerAvg\n",
    "from FLAlgorithms.trainmodel.models import *\n",
    "from FLAlgorithms.servers.servertrans import pFedTrans, Cluster\n",
    "from FLAlgorithms.servers.serverpFedHN import pFedHN\n",
    "\n",
    "from utils.plot_utils import *\n",
    "from kmeans_pytorch import kmeans\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpu = 0\n",
    "device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "model = CNNTargetPC(10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Running time:------------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 79.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numb samples of each label:\n",
      " [6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]\n",
      "L: 0\n",
      "L: 1\n",
      "L: 2\n",
      "L: 1\n",
      "L: 2\n",
      "L: 3\n",
      "L: 2\n",
      "L: 3\n",
      "L: 4\n",
      "L: 3\n",
      "L: 4\n",
      "L: 5\n",
      "L: 4\n",
      "L: 5\n",
      "L: 6\n",
      "L: 5\n",
      "L: 6\n",
      "L: 7\n",
      "L: 6\n",
      "L: 7\n",
      "L: 8\n",
      "L: 7\n",
      "L: 8\n",
      "L: 9\n",
      "L: 8\n",
      "L: 9\n",
      "L: 0\n",
      "L: 9\n",
      "L: 0\n",
      "L: 1\n",
      "L: 0\n",
      "L: 1\n",
      "L: 2\n",
      "L: 1\n",
      "L: 2\n",
      "L: 3\n",
      "L: 2\n",
      "L: 3\n",
      "L: 4\n",
      "L: 3\n",
      "L: 4\n",
      "L: 5\n",
      "L: 4\n",
      "L: 5\n",
      "L: 6\n",
      "L: 5\n",
      "L: 6\n",
      "L: 7\n",
      "L: 6\n",
      "L: 7\n",
      "L: 8\n",
      "L: 7\n",
      "L: 8\n",
      "L: 9\n",
      "L: 8\n",
      "L: 9\n",
      "L: 0\n",
      "L: 9\n",
      "L: 0\n",
      "L: 1\n",
      "IDX1: [60 60 60 60 60 60 60 60 60 60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:01<00:21,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 0 len data 1796 1766\n",
      "check len os user: 0 1 len data 3584 1788\n",
      "check len os user: 0 2 len data 4536 952\n",
      "check len os user: 1 0 len data 900 870\n",
      "check len os user:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:01<00:14,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 1 len data 1624 724\n",
      "check len os user: 1 2 len data 2794 1170\n",
      "check len os user: 2 0 len data 2246 2216\n",
      "check len os user: 2 1 len data 3346 1100\n",
      "check len os user:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:03<00:17,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 2 len data 4494 1148\n",
      "check len os user: 3 0 len data 1162 1132\n",
      "check len os user: 3 1 len data 1864 702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:03<00:16,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 3 2 len data 3418 1554\n",
      "check len os user: 4 0 len data 800 770\n",
      "check len os user: 4 1 len data 1802 1002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:04<00:11,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 4 2 len data 2936 1134\n",
      "check len os user: 5 0 len data 650 620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:05<00:12,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 5 1 len data 1706 1056\n",
      "check len os user: 5 2 len data 2612 906\n",
      "check len os user: 6 0 len data 918 888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:05<00:08,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 6 1 len data 1990 1072\n",
      "check len os user: 6 2 len data 3062 1072\n",
      "check len os user: 7 0 len data 672 642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:06<00:10,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 7 1 len data 1328 656\n",
      "check len os user: 7 2 len data 1954 626\n",
      "check len os user: 8 0 len data 1202 1172\n",
      "check len os user: 8 1 len data 1810 608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:07<00:07,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 8 2 len data 3186 1376\n",
      "check len os user: 9 0 len data 878 848\n",
      "check len os user: 9 1 len data 1928 1050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:08<00:09,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 9 2 len data 2648 720\n",
      "check len os user: 10 0 len data 1174 1144\n",
      "check len os user: 10 1 len data 2026 852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:09<00:06,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 10 2 len data 3076 1050\n",
      "check len os user: 11 0 len data 1136 1106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:09<00:03,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 11 2 len data 2028 892\n",
      "check len os user: 12 1 len data 1060 1030\n",
      "check len os user: 12 2 len data 1934 874\n",
      "check len os user: 13 1 len data"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:11<00:05,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 938 908\n",
      "check len os user: 13 2 len data 1572 634\n",
      "check len os user: 14 0 len data 1056 1026\n",
      "check len os user: 14 1 len data 2254 1198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:11<00:03,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 14 2 len data 2966 712\n",
      "check len os user: 15 0 len data 822 792\n",
      "check len os user: 15 1 len data 1724 902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:11<00:02,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 15 2 len data 2934 1210\n",
      "check len os user: 16 0 len data 1068 1038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:12<00:01,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 16 1 len data 2250 1182\n",
      "check len os user: 16 2 len data 3294 1044\n",
      "check len os user: 17 1 len data 884 854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 17 2 len data 1808 924\n",
      "check len os user: 18 0 len data 970 940\n",
      "check len os user: 18 1 len data 2170 1200\n",
      "check len os user: 19 0 len data 1150 1120\n",
      "IDX2: [5396 5396 5002 5384 5488 5860 5790 5072 5798 5386]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users / total users: 100  /  20\n",
      "Finished creating pFedHN server.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "gpu = 0\n",
    "device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "\n",
    "print(\"---------------Running time:------------\",)\n",
    "# Generate model\n",
    "model = 'cnn'\n",
    "dataset = 'Cifar10'\n",
    "\n",
    "model = CNNTargetPC().to(device), model\n",
    "algorithm = \"pFedTrans\"\n",
    "# select algorithm\n",
    "batch_size = 20\n",
    "learning_rate = 0.01\n",
    "personal_learning_rate = 0.01\n",
    "beta = 2\n",
    "lamda = 15\n",
    "num_glob_iters = 800\n",
    "local_epochs = 20\n",
    "optimizer = \"SGD\"\n",
    "numusers = 100 \n",
    "K = 5\n",
    "time1 = time.time()\n",
    "server = pFedHN(device, dataset, algorithm, model, batch_size, learning_rate,beta, lamda, num_glob_iters, local_epochs, optimizer, numusers, K, personal_learning_rate, 5)\n",
    "time2 = time.time()\n",
    "cost_time = time2-time1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "glob_iter = 0\n",
    "print(server.num_users)\n",
    "server.num_users = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20424/1728024860.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnode_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0muser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from tqdm import trange\n",
    "loss = []\n",
    "server.num_users = 5\n",
    "step_iter = trange(server.num_glob_iters)\n",
    "server.hnet.train()\n",
    "node_id = random.choice(range(server.num_users))\n",
    "\n",
    "user = server.users[node_id]\n",
    "weights =server.hnet(torch.tensor([node_id], dtype=torch.long).to(server.device))\n",
    "server.net.load_state_dict(weights)\n",
    "user.model.load_state_dict(weights)\n",
    "\n",
    "#previous acc and loss\n",
    "prv_acc, prv_loss, ns = user.test_acc_loss()\n",
    "step_iter.set_description(\n",
    "    f\"Step: {glob_iter+1}, User ID: {node_id}, Loss: {prv_loss:.4f},  Acc: {prv_acc:.4f}\"\n",
    ")\n",
    "# init inner optim\n",
    "inner_optimizer = torch.optim.SGD(\n",
    "    user.model.parameters(),lr=server.inner_lr,momentum=.9,\n",
    "    weight_decay=server.inner_wd\n",
    ")\n",
    "\n",
    "# storing theta_i for later calculating delta theta\n",
    "inner_state = OrderedDict({k: tensor.data for k, tensor in weights.items()})\n",
    "\n",
    "            \n",
    "for i in range(server.inner_steps):\n",
    "    user.model.train()\n",
    "    user.local_layers.train()\n",
    "    inner_optimizer.zero_grad()\n",
    "    server.optimizer.zero_grad()\n",
    "    user.local_optimizer.zero_grad()\n",
    "    #user.train(server.net)\n",
    "    X, y = user.get_next_train_batch()\n",
    "    inner_optimizer.step()\n",
    "    output = user.model(X)\n",
    "    pred = user.local_layers(output)\n",
    "    loss = user.loss(pred, y)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(user.model.parameters(), 50)\n",
    "    inner_optimizer.step()\n",
    "    user.local_optimizer.step()\n",
    "            \n",
    "server.optimizer.zero_grad()\n",
    "final_state = user.model.state_dict()\n",
    "\n",
    "delta_theta = OrderedDict({k: inner_state[k] - final_state[k] for k in weights.keys()})\n",
    "\n",
    "# calculating phi gradient\n",
    "hnet_grads = torch.autograd.grad(\n",
    "    list(weights.values()), server.hnet.parameters(), grad_outputs=list(delta_theta.values())\n",
    ")\n",
    "\n",
    "# update hnet weights\n",
    "for p, g in zip(server.hnet.parameters(), hnet_grads):\n",
    "    p.grad = g\n",
    "\n",
    "torch.nn.utils.clip_grad_norm_(server.hnet.parameters(), 50)\n",
    "server.optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Global Accurancy:  0.052375809935205186\n",
      "Average Global Trainning Accurancy:  0.051011195377392564\n",
      "Average Global Trainning Loss:  2.5896004028981583\n",
      "odict_values([tensor([[-0.0288,  0.0007,  0.0246,  ..., -0.0173,  0.0258, -0.0281],\n",
      "        [ 0.0296,  0.0215,  0.0313,  ..., -0.0277,  0.0247, -0.0163],\n",
      "        [-0.0142, -0.0299, -0.0086,  ..., -0.0127, -0.0299,  0.0194],\n",
      "        ...,\n",
      "        [-0.0193,  0.0165,  0.0279,  ..., -0.0141, -0.0239, -0.0185],\n",
      "        [-0.0209, -0.0055,  0.0129,  ...,  0.0038,  0.0349,  0.0241],\n",
      "        [-0.0167,  0.0257, -0.0204,  ..., -0.0065,  0.0138, -0.0138]]), tensor([-0.0019,  0.0304,  0.0117,  0.0350, -0.0017, -0.0129,  0.0314, -0.0224,\n",
      "         0.0167,  0.0302])])\n",
      "odict_values([tensor([[-0.0288,  0.0007,  0.0246,  ..., -0.0173,  0.0258, -0.0281],\n",
      "        [ 0.0296,  0.0215,  0.0313,  ..., -0.0277,  0.0247, -0.0163],\n",
      "        [-0.0142, -0.0299, -0.0086,  ..., -0.0127, -0.0299,  0.0194],\n",
      "        ...,\n",
      "        [-0.0193,  0.0165,  0.0279,  ..., -0.0141, -0.0239, -0.0185],\n",
      "        [-0.0209, -0.0054,  0.0129,  ...,  0.0038,  0.0349,  0.0241],\n",
      "        [-0.0167,  0.0257, -0.0204,  ..., -0.0065,  0.0138, -0.0138]]), tensor([ 0.0238,  0.0579,  0.0053,  0.0283, -0.0087, -0.0193,  0.0247, -0.0297,\n",
      "         0.0103,  0.0240])])\n",
      "Average Global Accurancy:  0.9281857451403888\n",
      "Average Global Trainning Accurancy:  0.9452871072589383\n",
      "Average Global Trainning Loss:  0.6457800791124955\n"
     ]
    }
   ],
   "source": [
    "server.evaluate()\n",
    "server.prev_per_values = []\n",
    "            \n",
    "server.attn_optimizer.zero_grad()\n",
    "\n",
    "print(server.users[0].model.state_dict().values())\n",
    "for i, user in enumerate(server.users):\n",
    "    if glob_iter != 0:\n",
    "        user.prev_per_values = [0]*len(user.per_values)\n",
    "        server.copy_value(user.per_values, user.prev_per_values, if_grad=True)\n",
    "    user.train(server.local_epochs)\n",
    "    #get user embedding vec\n",
    "    user.emb(server.emb_layer)\n",
    "\n",
    "print(server.users[0].model.state_dict().values())\n",
    "server.evaluate()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cpu..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 2it [00:00, 498.46it/s, center_shift=0.000000, iteration=2, tol=0.000100]\n",
      "2022-06-04 23:15:56,775 - server - INFO - iteration 0 cluster results:\n",
      "2022-06-04 23:15:56,776 - server - INFO - cluster 0 : f_00006 f_00007 f_00016 f_00009 f_00002 f_00013 f_00018 f_00019 \n",
      "2022-06-04 23:15:56,777 - server - INFO - cluster 1 : f_00008 f_00017 f_00013 f_00018 f_00005 f_00006 f_00015 f_00016 f_00005 f_00006 f_00015 f_00016 f_00006 f_00007 \n",
      "2022-06-04 23:15:56,777 - server - INFO - cluster 2 : f_00009 f_00018 f_00019 f_00011 f_00007 f_00017 f_00011 f_00016 \n",
      "2022-06-04 23:15:56,778 - server - INFO - cluster 3 : f_00000 f_00010 f_00011 f_00006 f_00007 f_00015 f_00004 f_00014 f_00000 f_00009 f_00010 f_00019 f_00002 f_00012 \n",
      "2022-06-04 23:15:56,779 - server - INFO - cluster 4 : f_00013 f_00000 f_00010 f_00019 f_00008 f_00009 f_00018 f_00019 f_00001 f_00005 f_00015 \n",
      "2022-06-04 23:15:56,779 - server - INFO - cluster 5 : f_00004 f_00014 f_00012 f_00013 f_00002 f_00003 f_00013 \n",
      "2022-06-04 23:15:56,780 - server - INFO - cluster 6 : f_00003 f_00001 f_00002 f_00012 f_00003 f_00014 \n",
      "2022-06-04 23:15:56,782 - server - INFO - cluster 7 : f_00015 f_00008 f_00017 f_00003 f_00004 f_00014 f_00004 \n",
      "2022-06-04 23:15:56,783 - server - INFO - cluster 8 : f_00001 f_00002 f_00012 f_00003 f_00004 f_00014 f_00001 f_00011 f_00012 f_00000 f_00001 f_00010 f_00011 \n",
      "2022-06-04 23:15:56,784 - server - INFO - cluster 9 : f_00005 f_00005 f_00016 f_00000 f_00010 f_00007 f_00008 f_00017 f_00018 f_00008 f_00009 f_00017 \n"
     ]
    }
   ],
   "source": [
    "server.form_cluster()\n",
    "server.logger.info(\"iteration {} cluster results:\".format(glob_iter))\n",
    "for idx, cluster in enumerate(server.clusters):\n",
    "    res = \"cluster {} : \".format(idx)\n",
    "\n",
    "    for user in cluster.users:\n",
    "        res += str(user.id)\n",
    "        res += \" \"\n",
    "    server.logger.info(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.info(\"he\")\n",
    "ch = logging.Handler()\n",
    "ch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[-0.0003,  0.0191, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0237,  0.0103, -0.0219,  ...,  0.0088, -0.0009,  0.0009],\n",
      "        [ 0.0144, -0.0336, -0.0346,  ..., -0.0222, -0.0025, -0.0138],\n",
      "        [-0.0195, -0.0118,  0.0230,  ..., -0.0202,  0.0172,  0.0355]],\n",
      "       device='cuda:0')), ('fc1.bias', tensor([ 0.0421,  0.0745, -0.0228, -0.0464, -0.0416, -0.0250,  0.0211,  0.0023,\n",
      "         0.0199,  0.0073], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(server.users[0].model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spfe\n"
     ]
    }
   ],
   "source": [
    "model_path = \"ne\"\n",
    "i = 10\n",
    "\n",
    "alg = \"pfe\"\n",
    "local_ep1 = 10\n",
    "print(\"s{}\".format(alg,local_ep1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "from utils.plot_utils import *\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "flag = 1\n",
    "if(flag==0): # plot for MNIST convex \n",
    "    numusers = 5\n",
    "    num_glob_iters = 800\n",
    "    dataset = \"Mnist\"\n",
    "    local_ep = [20,20,20,20]\n",
    "    lamda = [15,15,15,15]\n",
    "    learning_rate = [0.005, 0.005, 0.005, 0.005]\n",
    "    beta =  [1.0, 1.0, 0.001, 1.0]\n",
    "    batch_size = [20,20,20,20]\n",
    "    K = [5,5,5,5,5,5]\n",
    "    personal_learning_rate = [0.1,0.1,0.1,0.1]\n",
    "    algorithms = [ \"pFedMe_p\",\"pFedMe\",\"PerAvg_p\",\"FedAvg\"]\n",
    "    plot_summary_one_figure_mnist_Compare(num_users=numusers, loc_ep1=local_ep, Numb_Glob_Iters=num_glob_iters, lamb=lamda,\n",
    "                               learning_rate=learning_rate, beta = beta, algorithms_list=algorithms, batch_size=batch_size, dataset=dataset, k = K, personal_learning_rate = personal_learning_rate)\n",
    "\n",
    "if(flag==1): # plot for Synthetic covex\n",
    "    numusers = 10\n",
    "    num_glob_iters = 600\n",
    "    dataset = \"Synthetic\"\n",
    "    local_ep = [20,20,20]\n",
    "    lamda = [30,30,30]\n",
    "    learning_rate = [0.005, 0.005, 0.005]\n",
    "    beta =  [2.0, 2.0, 2.0]\n",
    "    batch_size = [20,20,20]\n",
    "    K = [5,5,5]\n",
    "    personal_learning_rate = [0.01,0.01,0.01] \n",
    "    algorithms = [ \"pFedMe_p\",\"pFedMe\",\"pFedTrans_p\"]\n",
    "    plot_summary_one_figure_synthetic_Compare(num_users=numusers, loc_ep1=local_ep, Numb_Glob_Iters=num_glob_iters, lamb=lamda,\n",
    "                               learning_rate=learning_rate, beta = beta, algorithms_list=algorithms, batch_size=batch_size, dataset=dataset, k = K, personal_learning_rate = personal_learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_values([tensor([[-0.0003,  0.0191, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0237,  0.0103, -0.0219,  ...,  0.0088, -0.0009,  0.0009],\n",
      "        [ 0.0144, -0.0336, -0.0346,  ..., -0.0222, -0.0025, -0.0138],\n",
      "        [-0.0195, -0.0118,  0.0230,  ..., -0.0202,  0.0172,  0.0355]],\n",
      "       device='cuda:0'), tensor([ 0.0421,  0.0745, -0.0228, -0.0464, -0.0416, -0.0250,  0.0211,  0.0023,\n",
      "         0.0199,  0.0073], device='cuda:0')])\n",
      "odict_values([tensor([[-0.0003,  0.0191, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0237,  0.0103, -0.0219,  ...,  0.0088, -0.0009,  0.0009],\n",
      "        [ 0.0144, -0.0336, -0.0346,  ..., -0.0222, -0.0025, -0.0138],\n",
      "        [-0.0195, -0.0118,  0.0230,  ..., -0.0202,  0.0172,  0.0355]],\n",
      "       device='cuda:0'), tensor([-0.0134,  0.0243, -0.0113, -0.0358, -0.0281,  0.0018,  0.0321,  0.0083,\n",
      "         0.0353,  0.0183], device='cuda:0')])\n",
      "Average Personal Accurancy:  0.70707343412527\n",
      "Average Personal Trainning Accurancy:  0.9689418562657999\n",
      "Average Personal Trainning Loss:  0.25998027301428767\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(server.users[0].model.state_dict().values())\n",
    "for cluster in server.clusters:\n",
    "    for user in cluster.users:\n",
    "        alpha = server.alpha_layer(user.emb_vec)\n",
    "        alpha = torch.sigmoid(alpha)\n",
    "        user.per_values = server.weighted_agg_model([cluster.per_values, user.per_values], [1-alpha,alpha])\n",
    "        user.merge_base_per_model()\n",
    "    cluster.merge_base_per_model()\n",
    "\n",
    "print(server.users[0].model.state_dict().values())\n",
    "server.evaluate_personalized_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, user in enumerate(server.users):\n",
    "    user.prev_per_values = [0]*len(user.per_values)\n",
    "    server.copy_value(user.per_values, user.prev_per_values, if_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0.0323,  0.0191,  0.0091,  ..., -0.0150,  0.0041,  0.0090],\n",
      "        [ 0.0253, -0.0168,  0.0191,  ...,  0.0348, -0.0115, -0.0151],\n",
      "        [-0.0091,  0.0183,  0.0120,  ...,  0.0353, -0.0333,  0.0139],\n",
      "        ...,\n",
      "        [ 0.0158, -0.0343, -0.0147,  ..., -0.0248,  0.0140,  0.0297],\n",
      "        [-0.0180,  0.0150,  0.0291,  ...,  0.0168, -0.0131,  0.0281],\n",
      "        [-0.0081, -0.0231, -0.0033,  ...,  0.0317, -0.0223, -0.0356]],\n",
      "       device='cuda:0'), tensor([-0.0037,  0.0056,  0.0282, -0.0248,  0.0320,  0.0182,  0.0168, -0.0324,\n",
      "         0.0074,  0.0250], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "print(server.users[0].prev_per_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep cost time : {} 0.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time1 = time.time()\n",
    "time2 = tijkjme.time()\n",
    "cost_time = time2-time1 \n",
    "print(\"ep cost time : {}\", str(cost_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, user in enumerate(server.users):\n",
    "    #if g_iter != 0:\n",
    "        #user.prev_per_values = [0]*len(user.per_values)\n",
    "        #server.copy_value(user.prev_per_values, user.per_values)\n",
    "    user.train(server.local_epochs)\n",
    "    #get user embedding vec\n",
    "    user.emb(server.emb_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0.0323,  0.0191,  0.0091,  ..., -0.0150,  0.0041,  0.0090],\n",
      "        [ 0.0253, -0.0168,  0.0191,  ...,  0.0348, -0.0115, -0.0151],\n",
      "        [-0.0091,  0.0183,  0.0120,  ...,  0.0353, -0.0333,  0.0139],\n",
      "        ...,\n",
      "        [ 0.0158, -0.0343, -0.0147,  ..., -0.0248,  0.0140,  0.0297],\n",
      "        [-0.0180,  0.0150,  0.0291,  ...,  0.0168, -0.0131,  0.0281],\n",
      "        [-0.0081, -0.0231, -0.0033,  ...,  0.0317, -0.0223, -0.0356]],\n",
      "       device='cuda:0'), tensor([-0.0037,  0.0056,  0.0282, -0.0248,  0.0320,  0.0182,  0.0168, -0.0324,\n",
      "         0.0074,  0.0250], device='cuda:0')]\n",
      "[tensor([[ 0.0323,  0.0191,  0.0091,  ..., -0.0150,  0.0041,  0.0090],\n",
      "        [ 0.0253, -0.0168,  0.0191,  ...,  0.0348, -0.0115, -0.0151],\n",
      "        [-0.0091,  0.0183,  0.0120,  ...,  0.0353, -0.0333,  0.0139],\n",
      "        ...,\n",
      "        [ 0.0158, -0.0343, -0.0147,  ..., -0.0248,  0.0140,  0.0297],\n",
      "        [-0.0180,  0.0150,  0.0291,  ...,  0.0168, -0.0131,  0.0281],\n",
      "        [-0.0081, -0.0231, -0.0033,  ...,  0.0316, -0.0223, -0.0356]],\n",
      "       device='cuda:0'), tensor([ 0.0205,  0.0334,  0.0209, -0.0309,  0.0258,  0.0128,  0.0096, -0.0392,\n",
      "         0.0011,  0.0181], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "user = server.users[0]\n",
    "print(user.prev_per_values)\n",
    "print(user.per_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7850]) tensor([[ 0.0323,  0.0191,  0.0091,  ..., -0.0392,  0.0011,  0.0181]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35956/425959943.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "criteria = nn.MSELoss()\n",
    "total_train = 0\n",
    "server.attn_optimizer.zero_grad()\n",
    "for user in server.users:\n",
    "    total_train += user.train_samples\n",
    "targets = None \n",
    "preds = None\n",
    "user = server.users[0]\n",
    "if preds == None and targets == None:\n",
    "    preds =  server.copy_value(nn.utils.parameters_to_vector(user.prev_per_values).unsqueeze(0), if_grad=True, if_tensor=True)\n",
    "    targets =  server.copy_value(nn.utils.parameters_to_vector(server.copy_value(user.per_values)).unsqueeze(0), if_tensor=True)\n",
    "else:\n",
    "    preds =  torch.cat((preds, nn.utils.parameters_to_vector(user.prev_per_values).unsqueeze(0)), 0)\n",
    "    targets =  torch.cat((targets, nn.utils.parameters_to_vector(server.copy_value(user.per_values)).unsqueeze(0)), 0)\n",
    "\n",
    "#for i, user in enumerate(server.users):\n",
    "\n",
    "    #ratio = user.train_samples / total_train\n",
    "    #if preds == None and targets == None:\n",
    "        #preds =  nn.utils.parameters_to_vector(user.prev_per_values).unsqueeze(0) \n",
    "        #targets =  nn.utils.parameters_to_vector(server.copy_value(user.per_values)).unsqueeze(0) \n",
    "    #else:\n",
    "        #preds =  torch.cat((preds, nn.utils.parameters_to_vector(user.prev_per_values).unsqueeze(0)), 0)\n",
    "        #targets =  torch.cat((targets, ratio * nn.utils.parameters_to_vector(server.copy_value(user.per_values)).unsqueeze(0)), 0)\n",
    "    #if i >= 5:\n",
    "        #break\n",
    "\n",
    "print(preds.size(), targets)\n",
    "loss = criteria(preds, targets)\n",
    "loss.backward()\n",
    "server.attn_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.attn_optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda:0..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 2it [00:00, 511.16it/s, center_shift=0.000000, iteration=2, tol=0.000100]\n"
     ]
    }
   ],
   "source": [
    "if g_iter % 5 == 0:\n",
    "    server.form_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in server.clusters:\n",
    "    cluster.avg_update_model()\n",
    "    cluster.emb(server.emb_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in server.clusters:\n",
    "    server.intra_cluster_agg(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.inter_cluster_agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0237,  0.0103, -0.0219,  ...,  0.0088, -0.0009,  0.0009],\n",
      "        [ 0.0144, -0.0336, -0.0346,  ..., -0.0222, -0.0025, -0.0138],\n",
      "        [-0.0195, -0.0118,  0.0230,  ..., -0.0202,  0.0172,  0.0355]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), tensor([-0.0200,  0.0342,  0.0464, -0.0196, -0.0387, -0.0235,  0.0221,  0.0032,\n",
      "         0.0200,  0.0073], device='cuda:0', grad_fn=<AddBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "user = server.users[1]\n",
    "print(user.per_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in server.clusters:\n",
    "    for user in cluster.users:\n",
    "        alpha = server.alpha_layer(user.emb_vec)\n",
    "        alpha = torch.sigmoid(alpha)\n",
    "        user.per_values = server.weighted_agg_model([cluster.per_values, user.per_values], [1-alpha,alpha], use_grad=True)\n",
    "        user.merge_base_per_model()\n",
    "    cluster.merge_base_per_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Global Accurancy:  0.828023758099352\n",
      "Average Global Trainning Accurancy:  0.8423618634886241\n",
      "Average Global Trainning Loss:  1.2799933238251626\n"
     ]
    }
   ],
   "source": [
    "server.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import torchviz\n",
    "from torchviz import make_dot\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'E:\\工具软件\\graphviz\\\\bin'\n",
    "print(os.environ[\"PATH\"])\n",
    "dot = make_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "293539041be06dcdbc1dd82d46367ecf96c14410c0014bf8043197a4a2571a26"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
