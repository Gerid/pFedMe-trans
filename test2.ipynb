{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "import FLAlgorithms\n",
    "import utils\n",
    "\n",
    "#for jupyter notebook reload modules\n",
    "import imp\n",
    "imp.reload(FLAlgorithms) \n",
    "imp.reload(utils) \n",
    "from FLAlgorithms.servers.serveravg import FedAvg\n",
    "\n",
    "from FLAlgorithms.servers.serverpFedMe import pFedMe\n",
    "from FLAlgorithms.servers.serverperavg import PerAvg\n",
    "from FLAlgorithms.trainmodel.models import *\n",
    "from FLAlgorithms.servers.servertrans import pFedTrans\n",
    "\n",
    "from utils.plot_utils import *\n",
    "from utils.attention import FullQKAttention\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "\n",
    "print(\"---------------Running time:------------\",)\n",
    "# Generate model\n",
    "model = 'mclr'\n",
    "dataset = 'Mnist'\n",
    "\n",
    "model = Mclr_Logistic().to(device), model\n",
    "algorithm = \"pFedMe\"\n",
    "# select algorithm\n",
    "batch_size = 20\n",
    "learning_rate = 0.01\n",
    "personal_learning_rate = 0.01\n",
    "beta = 2\n",
    "lamda = 15\n",
    "num_glob_iters = 800\n",
    "local_epochs = 20\n",
    "optimizer = \"SGD\"\n",
    "numusers = 5\n",
    "K = 5\n",
    "server = pFedTrans(device, dataset, algorithm, model, batch_size, learning_rate,beta, lamda, num_glob_iters, local_epochs, optimizer, numusers, K, personal_learning_rate, 5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.emb_layer.to(device)\n",
    "for user in server.users:\n",
    "    user.train(10)\n",
    "    user.net_values = [*user.model.state_dict().values()]\n",
    "    user.per_values = user.net_values[-2:]\n",
    "    value_vec = nn.utils.parameters_to_vector(user.per_values).clone()\n",
    "    user.emb_vec = server.emb_layer(value_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from kmeans_pytorch import kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data\n",
    "data_size, dims, num_clusters = 1000, 100, 10\n",
    "\n",
    "client_emb_list = [user.emb_vec.data.clone().reshape(1, -1) for user in server.users]\n",
    "print(len(client_emb_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client_emb_list = torch.cat(client_emb_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client_emb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_clusters = 10\n",
    "# kmeans\n",
    "cluster_res, cluster_centers = kmeans(\n",
    "    X=client_emb_list, num_clusters=num_clusters, distance='euclidean', device=torch.device('cuda:0')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for client_id, cluster_id in enumerate(cluster_res):\n",
    "    server.users[client_id].cluster_id = cluster_res[client_id] \n",
    "    server.clusters[cluster_id].users.append(server.users[client_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for cluster in server.clusters:\n",
    "    cluster.update_model(server.emb_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reformer_pytorch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((10, 1, 10)).to(device)\n",
    "y = torch.randn((10, 1, 10)).to(device)\n",
    "z = torch.randn((10, 1, 10)).to(device)\n",
    "\n",
    "embed_dim = 10\n",
    "num_heads = 5\n",
    "attn = nn.MultiheadAttention(embed_dim, num_heads).to(device)\n",
    "opt, weight = attn(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "attn = nn.MultiheadAttention(embed_dim, num_heads).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 128])\n",
      "tensor([[[0.1003, 0.0992, 0.1001, 0.0987, 0.1003, 0.1008, 0.1002, 0.0998,\n",
      "          0.1005, 0.1000],\n",
      "         [0.1005, 0.0992, 0.1001, 0.0989, 0.1004, 0.1009, 0.1001, 0.0996,\n",
      "          0.1005, 0.1000],\n",
      "         [0.1003, 0.0991, 0.1001, 0.0989, 0.1003, 0.1010, 0.1002, 0.0997,\n",
      "          0.1004, 0.1001],\n",
      "         [0.1006, 0.0992, 0.0999, 0.0989, 0.1003, 0.1008, 0.1001, 0.0997,\n",
      "          0.1006, 0.1000],\n",
      "         [0.1004, 0.0992, 0.0998, 0.0987, 0.1005, 0.1011, 0.1003, 0.0997,\n",
      "          0.1003, 0.1001],\n",
      "         [0.1004, 0.0990, 0.1001, 0.0987, 0.1004, 0.1009, 0.1002, 0.0996,\n",
      "          0.1005, 0.1001],\n",
      "         [0.1004, 0.0988, 0.1000, 0.0989, 0.1002, 0.1010, 0.1004, 0.0997,\n",
      "          0.1004, 0.1002],\n",
      "         [0.1004, 0.0990, 0.1002, 0.0989, 0.1002, 0.1008, 0.1004, 0.0995,\n",
      "          0.1003, 0.1003],\n",
      "         [0.1005, 0.0991, 0.1000, 0.0989, 0.1003, 0.1009, 0.1002, 0.0998,\n",
      "          0.1001, 0.1002],\n",
      "         [0.1004, 0.0989, 0.1001, 0.0990, 0.1002, 0.1009, 0.1004, 0.0997,\n",
      "          0.1003, 0.1002]]], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#intra_cluster_attn weigh        \n",
    "emb_dim = attn_dim = 128\n",
    "# position embedding :weight of total samples in cluster\n",
    "cluster_emb_list = [cluster.emb_vec.data.clone().reshape(1, -1) for cluster in server.clusters]\n",
    "x = torch.cat(cluster_emb_list, dim=0).unsqueeze(1)\n",
    "ln1 = torch.nn.LayerNorm(x.size()).to(device)\n",
    "x = ln1(x)\n",
    "print(x.size())\n",
    "l1 = nn.Linear(emb_dim, attn_dim).to(device)\n",
    "l2 = nn.Linear(emb_dim, attn_dim).to(device)\n",
    "a = l1(x)\n",
    "b = l2(x)\n",
    "c = torch.zeros_like(a)\n",
    "output, weight = attn(a, b, c)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 128])\n",
      "tensor([[[0.1016, 0.1019, 0.1063, 0.1005, 0.0981, 0.0958, 0.1010, 0.1004,\n",
      "          0.1019, 0.0924],\n",
      "         [0.0980, 0.0948, 0.0972, 0.1079, 0.0973, 0.1041, 0.0937, 0.0997,\n",
      "          0.0949, 0.1124],\n",
      "         [0.1008, 0.0996, 0.1008, 0.0933, 0.1097, 0.0984, 0.0969, 0.0943,\n",
      "          0.1027, 0.1034],\n",
      "         [0.0960, 0.0970, 0.1015, 0.0997, 0.1030, 0.1014, 0.1023, 0.1005,\n",
      "          0.1079, 0.0906],\n",
      "         [0.1034, 0.0964, 0.0966, 0.0999, 0.0973, 0.1046, 0.0915, 0.1065,\n",
      "          0.1041, 0.0997],\n",
      "         [0.1019, 0.0999, 0.1039, 0.1010, 0.1027, 0.0980, 0.0947, 0.1032,\n",
      "          0.1065, 0.0882],\n",
      "         [0.1024, 0.0974, 0.0950, 0.1016, 0.0994, 0.1049, 0.0952, 0.1025,\n",
      "          0.1063, 0.0953],\n",
      "         [0.1001, 0.0983, 0.0932, 0.1018, 0.0944, 0.1058, 0.1041, 0.1123,\n",
      "          0.0955, 0.0946],\n",
      "         [0.1001, 0.1044, 0.1016, 0.0929, 0.1044, 0.0966, 0.1066, 0.0942,\n",
      "          0.0988, 0.1005],\n",
      "         [0.1002, 0.1017, 0.0959, 0.0991, 0.0973, 0.1125, 0.0996, 0.0994,\n",
      "          0.1015, 0.0928]]], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 128\n",
    "attn_dim = 128\n",
    "x = torch.randn((10, 1, 128)).to(device)\n",
    "print(x.size())\n",
    "l1 = nn.Linear(emb_dim, attn_dim).to(device)\n",
    "l2 = nn.Linear(emb_dim, attn_dim).to(device)\n",
    "\n",
    "a = l1(x)\n",
    "b = l2(x)\n",
    "c = torch.ones_like(a)\n",
    "output, weight = attn(a, b, c)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_query = inter_query_weight(inter_attn_input)\n",
    "inter_key = inter_key_weight(inter_attn_input)\n",
    "inter_value = torch.ones_like(inter_query).to(device)\n",
    "#inter_key = inter_key_weight(inter_attn_input)\n",
    "#inter_value = inter_value_weight(inter_query)\n",
    "attn_output, attn_weight = attn(inter_query, inter_key, inter_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.ones((10, 1, 128)).to(device)\n",
    "output, weight = attn(x, y, z)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.ones((10, 1, 128)).to(device)\n",
    "attn_output, attn_weight = attn(x, y, z)\n",
    "print(attn_output)\n",
    "print(attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"---------------Running time:------------\",i)\n",
    "    # Generate model\n",
    "    model = 'mclr'\n",
    "    dataset = 'Mnist'\n",
    "\n",
    "    model = Mclr_Logistic().to(device), model\n",
    "    algorithm = \"pFedMe\"\n",
    "    # select algorithm\n",
    "    batch_size = 20\n",
    "    learning_rate = 0.01\n",
    "    personal_learning_rate = 0.01\n",
    "    beta = 2\n",
    "    lamda = 15\n",
    "    num_glob_iters = 800\n",
    "    local_epochs = 20\n",
    "    optimizer = \"SGD\"\n",
    "    numusers = 5\n",
    "    K = 5\n",
    "    if(algorithm == \"FedAvg\"):\n",
    "        server = FedAvg(device, dataset, algorithm, model, batch_size, learning_rate, beta, lamda, num_glob_iters, local_epochs, optimizer, numusers, i)\n",
    "        \n",
    "    if(algorithm == \"pFedMe\"):\n",
    "        server = pFedMe(device, dataset, algorithm, model, batch_size, learning_rate, beta, lamda, num_glob_iters, local_epochs, optimizer, numusers, K, personal_learning_rate, i)\n",
    "\n",
    "    if(algorithm == \"PerAvg\"):\n",
    "        server = PerAvg(device, dataset, algorithm, model, batch_size, learning_rate, beta, lamda, num_glob_iters, local_epochs, optimizer, numusers, i)\n",
    "\n",
    "    server.train()\n",
    "    server.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from kmeans_pytorch import kmeans\n",
    "\n",
    "# data\n",
    "data_size, dims, num_clusters = 1000, 100, 3\n",
    "x = np.random.randn(data_size, dims) / 6\n",
    "x = torch.from_numpy(x)\n",
    "\n",
    "# kmeans\n",
    "cluster_ids_x, cluster_centers = kmeans(\n",
    "    X=x, num_clusters=num_clusters, distance='euclidean', device=torch.device('cuda:0')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(cluster_ids_x, cluster_centers.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = 5\n",
    "\n",
    "average_data(num_users=numusers, loc_ep1=local_epochs, Numb_Glob_Iters=num_glob_iters, lamb=lamda,learning_rate=learning_rate, beta = beta, algorithms=\"pFedMe_p\", batch_size=batch_size, dataset=dataset, k = K, personal_learning_rate = personal_learning_rate,times = times)\n",
    "average_data(num_users=numusers, loc_ep1=local_epochs, Numb_Glob_Iters=num_glob_iters, lamb=lamda,learning_rate=learning_rate, beta = beta, algorithms=algorithm, batch_size=batch_size, dataset=dataset, k = K, personal_learning_rate = personal_learning_rate,times = times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user = server.users[0]\n",
    "model = user.model\n",
    "net_values = [*model.state_dict().values()]\n",
    "print(net_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(server.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x, y in server.users[0].testloaderfull:\n",
    "    x = x.to(user.device)\n",
    "    print(x.size())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Average data \n",
    "if(algorithm == \"PerAvg\"):\n",
    "    algorithm == \"PerAvg_p\"\n",
    "if(algorithm == \"pFedMe\"):\n",
    "    average_data(num_users=numusers, loc_ep1=local_epochs, Numb_Glob_Iters=num_glob_iters, lamb=lamda,learning_rate=learning_rate, beta = beta, algorithms=\"pFedMe_p\", batch_size=batch_size, dataset=dataset, k = K, personal_learning_rate = personal_learning_rate,times = times)\n",
    "average_data(num_users=numusers, loc_ep1=local_epochs, Numb_Glob_Iters=num_glob_iters, lamb=lamda,learning_rate=learning_rate, beta = beta, algorithms=algorithm, batch_size=batch_size, dataset=dataset, k = K, personal_learning_rate = personal_learning_rate,times = times)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "293539041be06dcdbc1dd82d46367ecf96c14410c0014bf8043197a4a2571a26"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
