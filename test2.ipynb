{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x210c4d64ed0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from kmeans_pytorch import kmeans\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "import FLAlgorithms\n",
    "import utils\n",
    "\n",
    "#for jupyter notebook reload modules\n",
    "import imp\n",
    "imp.reload(FLAlgorithms) \n",
    "imp.reload(utils) \n",
    "from FLAlgorithms.servers.serveravg import FedAvg\n",
    "from FLAlgorithms.servers.serverpFedMe import pFedMe\n",
    "from FLAlgorithms.servers.serverperavg import PerAvg\n",
    "from FLAlgorithms.trainmodel.models import *\n",
    "from FLAlgorithms.servers.servertrans import pFedTrans, Cluster\n",
    "from FLAlgorithms.servers.serverpFedHN import pFedHN\n",
    "\n",
    "from utils.plot_utils import *\n",
    "from kmeans_pytorch import kmeans\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpu = 0\n",
    "device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "model = CNNTargetPC(10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Running time:------------\n",
      "Number of users / total users: 100  /  20\n",
      "Finished creating pFedHN server.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "gpu = 0\n",
    "device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "\n",
    "print(\"---------------Running time:------------\",)\n",
    "# Generate model\n",
    "model = 'cnn'\n",
    "dataset = 'Cifar10'\n",
    "\n",
    "model = CNNTargetPC().to(device), model\n",
    "algorithm = \"pFedTrans\"\n",
    "# select algorithm\n",
    "batch_size = 20\n",
    "learning_rate = 0.01\n",
    "personal_learning_rate = 0.01\n",
    "beta = 2\n",
    "lamda = 15\n",
    "num_glob_iters = 800\n",
    "local_epochs = 20\n",
    "optimizer = \"SGD\"\n",
    "numusers = 100 \n",
    "K = 5\n",
    "time1 = time.time()\n",
    "server = pFedHN(device, dataset, algorithm, model, batch_size, learning_rate,beta, lamda, num_glob_iters, local_epochs, optimizer, numusers, K, personal_learning_rate, 5)\n",
    "time2 = time.time()\n",
    "cost_time = time2-time1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = server.users[0]\n",
    "print(user.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/800 [04:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 3, 5, 5], expected input[195, 1, 28, 28] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5436/1090923839.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#previous acc and loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprv_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprv_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_acc_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m step_iter.set_description(\n\u001b[0;32m     16\u001b[0m     \u001b[1;34mf\"Step: {glob_iter+1}, User ID: {node_id}, Loss: {prv_loss:.4f},  Acc: {prv_acc:.4f}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fang\\Documents\\gitspace\\pFedMe-trans\\FLAlgorithms\\users\\userpFedHN.py\u001b[0m in \u001b[0;36mtest_acc_loss\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mtest_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fang\\miniconda\\envs\\phd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fang\\Documents\\gitspace\\pFedMe-trans\\FLAlgorithms\\trainmodel\\models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fang\\miniconda\\envs\\phd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fang\\miniconda\\envs\\phd\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fang\\miniconda\\envs\\phd\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 443\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 3, 5, 5], expected input[195, 1, 28, 28] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from tqdm import trange\n",
    "loss = []\n",
    "step_iter = trange(server.num_glob_iters)\n",
    "server.hnet.train()\n",
    "node_id = random.choice(range(server.num_users))\n",
    "\n",
    "user = server.users[node_id]\n",
    "weights =server.hnet(torch.tensor([node_id], dtype=torch.long).to(server.device))\n",
    "server.net.load_state_dict(weights)\n",
    "user.model.load_state_dict(weights)\n",
    "\n",
    "#previous acc and loss\n",
    "prv_acc, prv_loss, ns = user.test_acc_loss()\n",
    "step_iter.set_description(\n",
    "    f\"Step: {glob_iter+1}, User ID: {node_id}, Loss: {prv_loss:.4f},  Acc: {prv_acc:.4f}\"\n",
    ")\n",
    "# init inner optim\n",
    "inner_optimizer = torch.optim.SGD(\n",
    "    user.model.parameters(),lr=server.inner_lr,momentum=.9,\n",
    "    weight_decay=server.inner_wd\n",
    ")\n",
    "\n",
    "# storing theta_i for later calculating delta theta\n",
    "inner_state = OrderedDict({k: tensor.data for k, tensor in weights.items()})\n",
    "\n",
    "            \n",
    "for i in range(server.inner_steps):\n",
    "    user.model.train()\n",
    "    user.local_layers.train()\n",
    "    inner_optimizer.zero_grad()\n",
    "    server.optimizer.zero_grad()\n",
    "    user.local_optimizer.zero_grad()\n",
    "    #user.train(server.net)\n",
    "    X, y = user.get_next_train_batch()\n",
    "    inner_optimizer.step()\n",
    "    output = user.model(X)\n",
    "    pred = user.local_layers(output)\n",
    "    loss = user.loss(pred, y)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(user.model.parameters(), 50)\n",
    "    inner_optimizer.step()\n",
    "    user.local_optimizer.step()\n",
    "            \n",
    "server.optimizer.zero_grad()\n",
    "final_state = user.model.state_dict()\n",
    "\n",
    "delta_theta = OrderedDict({k: inner_state[k] - final_state[k] for k in weights.keys()})\n",
    "\n",
    "# calculating phi gradient\n",
    "hnet_grads = torch.autograd.grad(\n",
    "    list(weights.values()), server.hnet.parameters(), grad_outputs=list(delta_theta.values())\n",
    ")\n",
    "\n",
    "# update hnet weights\n",
    "for p, g in zip(server.hnet.parameters(), hnet_grads):\n",
    "    p.grad = g\n",
    "\n",
    "torch.nn.utils.clip_grad_norm_(server.hnet.parameters(), 50)\n",
    "server.optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Global Accurancy:  0.052375809935205186\n",
      "Average Global Trainning Accurancy:  0.051011195377392564\n",
      "Average Global Trainning Loss:  2.5896004028981583\n",
      "odict_values([tensor([[-0.0288,  0.0007,  0.0246,  ..., -0.0173,  0.0258, -0.0281],\n",
      "        [ 0.0296,  0.0215,  0.0313,  ..., -0.0277,  0.0247, -0.0163],\n",
      "        [-0.0142, -0.0299, -0.0086,  ..., -0.0127, -0.0299,  0.0194],\n",
      "        ...,\n",
      "        [-0.0193,  0.0165,  0.0279,  ..., -0.0141, -0.0239, -0.0185],\n",
      "        [-0.0209, -0.0055,  0.0129,  ...,  0.0038,  0.0349,  0.0241],\n",
      "        [-0.0167,  0.0257, -0.0204,  ..., -0.0065,  0.0138, -0.0138]]), tensor([-0.0019,  0.0304,  0.0117,  0.0350, -0.0017, -0.0129,  0.0314, -0.0224,\n",
      "         0.0167,  0.0302])])\n",
      "odict_values([tensor([[-0.0288,  0.0007,  0.0246,  ..., -0.0173,  0.0258, -0.0281],\n",
      "        [ 0.0296,  0.0215,  0.0313,  ..., -0.0277,  0.0247, -0.0163],\n",
      "        [-0.0142, -0.0299, -0.0086,  ..., -0.0127, -0.0299,  0.0194],\n",
      "        ...,\n",
      "        [-0.0193,  0.0165,  0.0279,  ..., -0.0141, -0.0239, -0.0185],\n",
      "        [-0.0209, -0.0054,  0.0129,  ...,  0.0038,  0.0349,  0.0241],\n",
      "        [-0.0167,  0.0257, -0.0204,  ..., -0.0065,  0.0138, -0.0138]]), tensor([ 0.0238,  0.0579,  0.0053,  0.0283, -0.0087, -0.0193,  0.0247, -0.0297,\n",
      "         0.0103,  0.0240])])\n",
      "Average Global Accurancy:  0.9281857451403888\n",
      "Average Global Trainning Accurancy:  0.9452871072589383\n",
      "Average Global Trainning Loss:  0.6457800791124955\n"
     ]
    }
   ],
   "source": [
    "server.evaluate()\n",
    "server.prev_per_values = []\n",
    "            \n",
    "server.attn_optimizer.zero_grad()\n",
    "\n",
    "print(server.users[0].model.state_dict().values())\n",
    "for i, user in enumerate(server.users):\n",
    "    if glob_iter != 0:\n",
    "        user.prev_per_values = [0]*len(user.per_values)\n",
    "        server.copy_value(user.per_values, user.prev_per_values, if_grad=True)\n",
    "    user.train(server.local_epochs)\n",
    "    #get user embedding vec\n",
    "    user.emb(server.emb_layer)\n",
    "\n",
    "print(server.users[0].model.state_dict().values())\n",
    "server.evaluate()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cpu..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 2it [00:00, 498.46it/s, center_shift=0.000000, iteration=2, tol=0.000100]\n",
      "2022-06-04 23:15:56,775 - server - INFO - iteration 0 cluster results:\n",
      "2022-06-04 23:15:56,776 - server - INFO - cluster 0 : f_00006 f_00007 f_00016 f_00009 f_00002 f_00013 f_00018 f_00019 \n",
      "2022-06-04 23:15:56,777 - server - INFO - cluster 1 : f_00008 f_00017 f_00013 f_00018 f_00005 f_00006 f_00015 f_00016 f_00005 f_00006 f_00015 f_00016 f_00006 f_00007 \n",
      "2022-06-04 23:15:56,777 - server - INFO - cluster 2 : f_00009 f_00018 f_00019 f_00011 f_00007 f_00017 f_00011 f_00016 \n",
      "2022-06-04 23:15:56,778 - server - INFO - cluster 3 : f_00000 f_00010 f_00011 f_00006 f_00007 f_00015 f_00004 f_00014 f_00000 f_00009 f_00010 f_00019 f_00002 f_00012 \n",
      "2022-06-04 23:15:56,779 - server - INFO - cluster 4 : f_00013 f_00000 f_00010 f_00019 f_00008 f_00009 f_00018 f_00019 f_00001 f_00005 f_00015 \n",
      "2022-06-04 23:15:56,779 - server - INFO - cluster 5 : f_00004 f_00014 f_00012 f_00013 f_00002 f_00003 f_00013 \n",
      "2022-06-04 23:15:56,780 - server - INFO - cluster 6 : f_00003 f_00001 f_00002 f_00012 f_00003 f_00014 \n",
      "2022-06-04 23:15:56,782 - server - INFO - cluster 7 : f_00015 f_00008 f_00017 f_00003 f_00004 f_00014 f_00004 \n",
      "2022-06-04 23:15:56,783 - server - INFO - cluster 8 : f_00001 f_00002 f_00012 f_00003 f_00004 f_00014 f_00001 f_00011 f_00012 f_00000 f_00001 f_00010 f_00011 \n",
      "2022-06-04 23:15:56,784 - server - INFO - cluster 9 : f_00005 f_00005 f_00016 f_00000 f_00010 f_00007 f_00008 f_00017 f_00018 f_00008 f_00009 f_00017 \n"
     ]
    }
   ],
   "source": [
    "server.form_cluster()\n",
    "server.logger.info(\"iteration {} cluster results:\".format(glob_iter))\n",
    "for idx, cluster in enumerate(server.clusters):\n",
    "    res = \"cluster {} : \".format(idx)\n",
    "\n",
    "    for user in cluster.users:\n",
    "        res += str(user.id)\n",
    "        res += \" \"\n",
    "    server.logger.info(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.info(\"he\")\n",
    "ch = logging.Handler()\n",
    "ch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[-0.0003,  0.0191, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0237,  0.0103, -0.0219,  ...,  0.0088, -0.0009,  0.0009],\n",
      "        [ 0.0144, -0.0336, -0.0346,  ..., -0.0222, -0.0025, -0.0138],\n",
      "        [-0.0195, -0.0118,  0.0230,  ..., -0.0202,  0.0172,  0.0355]],\n",
      "       device='cuda:0')), ('fc1.bias', tensor([ 0.0421,  0.0745, -0.0228, -0.0464, -0.0416, -0.0250,  0.0211,  0.0023,\n",
      "         0.0199,  0.0073], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(server.users[0].model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spfe\n"
     ]
    }
   ],
   "source": [
    "model_path = \"ne\"\n",
    "i = 10\n",
    "\n",
    "alg = \"pfe\"\n",
    "local_ep1 = 10\n",
    "print(\"s{}\".format(alg,local_ep1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "from utils.plot_utils import *\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "flag = 1\n",
    "if(flag==0): # plot for MNIST convex \n",
    "    numusers = 5\n",
    "    num_glob_iters = 800\n",
    "    dataset = \"Mnist\"\n",
    "    local_ep = [20,20,20,20]\n",
    "    lamda = [15,15,15,15]\n",
    "    learning_rate = [0.005, 0.005, 0.005, 0.005]\n",
    "    beta =  [1.0, 1.0, 0.001, 1.0]\n",
    "    batch_size = [20,20,20,20]\n",
    "    K = [5,5,5,5,5,5]\n",
    "    personal_learning_rate = [0.1,0.1,0.1,0.1]\n",
    "    algorithms = [ \"pFedMe_p\",\"pFedMe\",\"PerAvg_p\",\"FedAvg\"]\n",
    "    plot_summary_one_figure_mnist_Compare(num_users=numusers, loc_ep1=local_ep, Numb_Glob_Iters=num_glob_iters, lamb=lamda,\n",
    "                               learning_rate=learning_rate, beta = beta, algorithms_list=algorithms, batch_size=batch_size, dataset=dataset, k = K, personal_learning_rate = personal_learning_rate)\n",
    "\n",
    "if(flag==1): # plot for Synthetic covex\n",
    "    numusers = 10\n",
    "    num_glob_iters = 600\n",
    "    dataset = \"Synthetic\"\n",
    "    local_ep = [20,20,20]\n",
    "    lamda = [30,30,30]\n",
    "    learning_rate = [0.005, 0.005, 0.005]\n",
    "    beta =  [2.0, 2.0, 2.0]\n",
    "    batch_size = [20,20,20]\n",
    "    K = [5,5,5]\n",
    "    personal_learning_rate = [0.01,0.01,0.01] \n",
    "    algorithms = [ \"pFedMe_p\",\"pFedMe\",\"pFedTrans_p\"]\n",
    "    plot_summary_one_figure_synthetic_Compare(num_users=numusers, loc_ep1=local_ep, Numb_Glob_Iters=num_glob_iters, lamb=lamda,\n",
    "                               learning_rate=learning_rate, beta = beta, algorithms_list=algorithms, batch_size=batch_size, dataset=dataset, k = K, personal_learning_rate = personal_learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_values([tensor([[-0.0003,  0.0191, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0237,  0.0103, -0.0219,  ...,  0.0088, -0.0009,  0.0009],\n",
      "        [ 0.0144, -0.0336, -0.0346,  ..., -0.0222, -0.0025, -0.0138],\n",
      "        [-0.0195, -0.0118,  0.0230,  ..., -0.0202,  0.0172,  0.0355]],\n",
      "       device='cuda:0'), tensor([ 0.0421,  0.0745, -0.0228, -0.0464, -0.0416, -0.0250,  0.0211,  0.0023,\n",
      "         0.0199,  0.0073], device='cuda:0')])\n",
      "odict_values([tensor([[-0.0003,  0.0191, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0237,  0.0103, -0.0219,  ...,  0.0088, -0.0009,  0.0009],\n",
      "        [ 0.0144, -0.0336, -0.0346,  ..., -0.0222, -0.0025, -0.0138],\n",
      "        [-0.0195, -0.0118,  0.0230,  ..., -0.0202,  0.0172,  0.0355]],\n",
      "       device='cuda:0'), tensor([-0.0134,  0.0243, -0.0113, -0.0358, -0.0281,  0.0018,  0.0321,  0.0083,\n",
      "         0.0353,  0.0183], device='cuda:0')])\n",
      "Average Personal Accurancy:  0.70707343412527\n",
      "Average Personal Trainning Accurancy:  0.9689418562657999\n",
      "Average Personal Trainning Loss:  0.25998027301428767\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(server.users[0].model.state_dict().values())\n",
    "for cluster in server.clusters:\n",
    "    for user in cluster.users:\n",
    "        alpha = server.alpha_layer(user.emb_vec)\n",
    "        alpha = torch.sigmoid(alpha)\n",
    "        user.per_values = server.weighted_agg_model([cluster.per_values, user.per_values], [1-alpha,alpha])\n",
    "        user.merge_base_per_model()\n",
    "    cluster.merge_base_per_model()\n",
    "\n",
    "print(server.users[0].model.state_dict().values())\n",
    "server.evaluate_personalized_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, user in enumerate(server.users):\n",
    "    user.prev_per_values = [0]*len(user.per_values)\n",
    "    server.copy_value(user.per_values, user.prev_per_values, if_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0.0323,  0.0191,  0.0091,  ..., -0.0150,  0.0041,  0.0090],\n",
      "        [ 0.0253, -0.0168,  0.0191,  ...,  0.0348, -0.0115, -0.0151],\n",
      "        [-0.0091,  0.0183,  0.0120,  ...,  0.0353, -0.0333,  0.0139],\n",
      "        ...,\n",
      "        [ 0.0158, -0.0343, -0.0147,  ..., -0.0248,  0.0140,  0.0297],\n",
      "        [-0.0180,  0.0150,  0.0291,  ...,  0.0168, -0.0131,  0.0281],\n",
      "        [-0.0081, -0.0231, -0.0033,  ...,  0.0317, -0.0223, -0.0356]],\n",
      "       device='cuda:0'), tensor([-0.0037,  0.0056,  0.0282, -0.0248,  0.0320,  0.0182,  0.0168, -0.0324,\n",
      "         0.0074,  0.0250], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "print(server.users[0].prev_per_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep cost time : {} 0.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time1 = time.time()\n",
    "time2 = tijkjme.time()\n",
    "cost_time = time2-time1 \n",
    "print(\"ep cost time : {}\", str(cost_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, user in enumerate(server.users):\n",
    "    #if g_iter != 0:\n",
    "        #user.prev_per_values = [0]*len(user.per_values)\n",
    "        #server.copy_value(user.prev_per_values, user.per_values)\n",
    "    user.train(server.local_epochs)\n",
    "    #get user embedding vec\n",
    "    user.emb(server.emb_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0.0323,  0.0191,  0.0091,  ..., -0.0150,  0.0041,  0.0090],\n",
      "        [ 0.0253, -0.0168,  0.0191,  ...,  0.0348, -0.0115, -0.0151],\n",
      "        [-0.0091,  0.0183,  0.0120,  ...,  0.0353, -0.0333,  0.0139],\n",
      "        ...,\n",
      "        [ 0.0158, -0.0343, -0.0147,  ..., -0.0248,  0.0140,  0.0297],\n",
      "        [-0.0180,  0.0150,  0.0291,  ...,  0.0168, -0.0131,  0.0281],\n",
      "        [-0.0081, -0.0231, -0.0033,  ...,  0.0317, -0.0223, -0.0356]],\n",
      "       device='cuda:0'), tensor([-0.0037,  0.0056,  0.0282, -0.0248,  0.0320,  0.0182,  0.0168, -0.0324,\n",
      "         0.0074,  0.0250], device='cuda:0')]\n",
      "[tensor([[ 0.0323,  0.0191,  0.0091,  ..., -0.0150,  0.0041,  0.0090],\n",
      "        [ 0.0253, -0.0168,  0.0191,  ...,  0.0348, -0.0115, -0.0151],\n",
      "        [-0.0091,  0.0183,  0.0120,  ...,  0.0353, -0.0333,  0.0139],\n",
      "        ...,\n",
      "        [ 0.0158, -0.0343, -0.0147,  ..., -0.0248,  0.0140,  0.0297],\n",
      "        [-0.0180,  0.0150,  0.0291,  ...,  0.0168, -0.0131,  0.0281],\n",
      "        [-0.0081, -0.0231, -0.0033,  ...,  0.0316, -0.0223, -0.0356]],\n",
      "       device='cuda:0'), tensor([ 0.0205,  0.0334,  0.0209, -0.0309,  0.0258,  0.0128,  0.0096, -0.0392,\n",
      "         0.0011,  0.0181], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "user = server.users[0]\n",
    "print(user.prev_per_values)\n",
    "print(user.per_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7850]) tensor([[ 0.0323,  0.0191,  0.0091,  ..., -0.0392,  0.0011,  0.0181]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35956/425959943.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MSI\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "criteria = nn.MSELoss()\n",
    "total_train = 0\n",
    "server.attn_optimizer.zero_grad()\n",
    "for user in server.users:\n",
    "    total_train += user.train_samples\n",
    "targets = None \n",
    "preds = None\n",
    "user = server.users[0]\n",
    "if preds == None and targets == None:\n",
    "    preds =  server.copy_value(nn.utils.parameters_to_vector(user.prev_per_values).unsqueeze(0), if_grad=True, if_tensor=True)\n",
    "    targets =  server.copy_value(nn.utils.parameters_to_vector(server.copy_value(user.per_values)).unsqueeze(0), if_tensor=True)\n",
    "else:\n",
    "    preds =  torch.cat((preds, nn.utils.parameters_to_vector(user.prev_per_values).unsqueeze(0)), 0)\n",
    "    targets =  torch.cat((targets, nn.utils.parameters_to_vector(server.copy_value(user.per_values)).unsqueeze(0)), 0)\n",
    "\n",
    "#for i, user in enumerate(server.users):\n",
    "\n",
    "    #ratio = user.train_samples / total_train\n",
    "    #if preds == None and targets == None:\n",
    "        #preds =  nn.utils.parameters_to_vector(user.prev_per_values).unsqueeze(0) \n",
    "        #targets =  nn.utils.parameters_to_vector(server.copy_value(user.per_values)).unsqueeze(0) \n",
    "    #else:\n",
    "        #preds =  torch.cat((preds, nn.utils.parameters_to_vector(user.prev_per_values).unsqueeze(0)), 0)\n",
    "        #targets =  torch.cat((targets, ratio * nn.utils.parameters_to_vector(server.copy_value(user.per_values)).unsqueeze(0)), 0)\n",
    "    #if i >= 5:\n",
    "        #break\n",
    "\n",
    "print(preds.size(), targets)\n",
    "loss = criteria(preds, targets)\n",
    "loss.backward()\n",
    "server.attn_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.attn_optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda:0..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 2it [00:00, 511.16it/s, center_shift=0.000000, iteration=2, tol=0.000100]\n"
     ]
    }
   ],
   "source": [
    "if g_iter % 5 == 0:\n",
    "    server.form_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in server.clusters:\n",
    "    cluster.avg_update_model()\n",
    "    cluster.emb(server.emb_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in server.clusters:\n",
    "    server.intra_cluster_agg(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.inter_cluster_agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0237,  0.0103, -0.0219,  ...,  0.0088, -0.0009,  0.0009],\n",
      "        [ 0.0144, -0.0336, -0.0346,  ..., -0.0222, -0.0025, -0.0138],\n",
      "        [-0.0195, -0.0118,  0.0230,  ..., -0.0202,  0.0172,  0.0355]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), tensor([-0.0200,  0.0342,  0.0464, -0.0196, -0.0387, -0.0235,  0.0221,  0.0032,\n",
      "         0.0200,  0.0073], device='cuda:0', grad_fn=<AddBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "user = server.users[1]\n",
    "print(user.per_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in server.clusters:\n",
    "    for user in cluster.users:\n",
    "        alpha = server.alpha_layer(user.emb_vec)\n",
    "        alpha = torch.sigmoid(alpha)\n",
    "        user.per_values = server.weighted_agg_model([cluster.per_values, user.per_values], [1-alpha,alpha], use_grad=True)\n",
    "        user.merge_base_per_model()\n",
    "    cluster.merge_base_per_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Global Accurancy:  0.828023758099352\n",
      "Average Global Trainning Accurancy:  0.8423618634886241\n",
      "Average Global Trainning Loss:  1.2799933238251626\n"
     ]
    }
   ],
   "source": [
    "server.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import torchviz\n",
    "from torchviz import make_dot\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'E:\\工具软件\\graphviz\\\\bin'\n",
    "print(os.environ[\"PATH\"])\n",
    "dot = make_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1ccd20b5258b628543699ad079c0d4ef32c5d2797833541d65624ed3b1ba210"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('phd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
